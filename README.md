Online Appendix for *"Enforcement Policy in a Dynamic Theory of Deterrence"*
============================================================================

The simulator
=============
The number of states in $\boldsymbol{Z}$ for $z=1$ and $N\leq 100$ is manageable, meaning it is feasible to produce the associated transition matrix $\boldsymbol{T}$ and use it to directly compute $\boldsymbol{D}$. But for $z>1$ and larger $N$ we must use numerical simulation methods. We now describe the simulation and our benchmarking efforts.

In each period of the simulation: Initiating a simulation requires values for: the quantity of enforcement resources ($R$); people's look back horizon ($z$); the number of potential violators ($N$); the mean ($\mu$) and standard deviation ($\sigma$) of the private gains from committing a violation; the probability a violator is apprehended if investigated ($\gamma$); the utility penalty paid by apprehended violators ($F$); and, the population's Bayesian priors regarding apprehension ($\alpha $, $\beta $). Unless noted, we set these parameters according to our baseline model.

* Every potential violator is assigned a $g$ drawn randomly from $\phi(g)$.
*Using the current z-history, a common subjective probability of apprehension $q$ is calculated.
* People choose to violate if and only if their $g^{t}_{i}\geq q^{t} \cdot F$.
* Violators are apprehended with probability $P(R^{t}, v^{t})$, where $v^{t}$ is the number of violators in the current period.
* The number of violations, apprehensions, and the sum of the gains from violating, the sum of the realized draws on $\phi(g)$ for those who violated, are recorded.
* The frequency distribution of violations and apprehensions is calculated for the current period and recorded.
* The z-history is updated and the simulation moves to the next period.

We calculate the frequency distribution of violations and apprehensions. The frequency distribution of violations is $\boldsymbol{FV}(nv_{0}^{t},nv_{1}^{t},\dotsc ,nv_{N}^{t})$, where $nv_{j}^{t}$ is the number of instances in the first $t$ periods of the simulation in which there were exactly $j$ violations. The frequency distribution of apprehensions is $\boldsymbol{FA}(na_{0}^{t},na_{1}^{t},\dotsc ,na_{N}^{t})$, where $na_{j}^{t}$ is the number of instances in the first $t$ periods of the simulation in which there were exactly $j$ apprehensions. 

The simulation proceeds in blocks of $50,000$ ticks. At the end of each block $n$ we use $\boldsymbol{FV}$ to calculate the relative frequency distribution of violations $\boldsymbol{RFV}^{n}=(rv_{0}^{n},\dotsc ,rv_{N}^{n})$, where $rv_{j}^{n}$ is the proportion of periods in which there were exactly $j$ violations in the first $50000\cdot n$ periods of the simulation. As $n$ approaches infinity  $\boldsymbol{RFV}^{n}$ approaches $\widehat{\boldsymbol{D}}$. The question is: How large must $n$ be to ensure that $\boldsymbol{RFV}^{n}$ is a \textit{good} approximation of $\widehat{\boldsymbol{D}}$? To answer this we need a convergence criterion. 

At the end of each simulation block, for all $n\geq 2$, we calculate the following test statistic:
\[TEST^{n}=\sum_{j=0}^{j=N}|rv_{j}^{n}-rv_{j}^{n-1}|\]
$TEST^{n}$ is a measure, decreasing in $n$, of the \textit{distance} between $\boldsymbol{RFV}^{n}$ and $\boldsymbol{RFV}^{n-1}$. $TEST^{n}$ does not decline monotonically, which complicates any test of convergence. After extensive experimentation we settled on the following criterion: we require that $TEST^{n} \leq 0.01$ for $5$ consecutive blocks of $n$. Denoting the minimum $n$ for which the convergence criterion is satisfied by $n^{\ast }$ yields our approximation of the stationary distribution, $\boldsymbol{RFV}^{n^{\ast }} \sim \widehat{\boldsymbol{D}}$.\footnote{\ An important confounding issue for finding $n^*$ is that the \textit{shape} of $\widehat{\boldsymbol{D}}$ changes dramatically as $R$ changes. For low values of $R$, $\widehat{\boldsymbol{D}}$ is unimodal and right skewed, for high values unimodal and left skewed, for intermediate values it becomes bimodal. This renders $n^*$ non-monotonic in R (see the last row of Table A.1).}

Our convergence simulator has some limitations over certain ranges of some key parameter values, in particular those that comprise the composite parameter $\frac{\sigma}{(\gamma \cdot F)}$ defined in Section \ref{section:CLIFFSET}. If $\frac{\sigma}{(\gamma \cdot F)}$ is small the $n$ required to achieve a good approximation of the stationary distribution is actually much larger than the $n^{\ast}$ generated from the convergence criteria. This is because of the persistence of the q-attractors. For these parameterizations the simulator is highly sensitive to initial conditions, staying for a very long time (sometimes the entire simulation) in only one of the q-attractors and rarely (if ever for some convergence simulations around the cliff) transitions to the other q-attractor, even though the stationary distribution is bi-model in these parameter values. This yields a poor estimate of the stationary distribution even though the convergence simulator may have taken a very long time to converge. More accurate estimations of the stationary distribution for such parameter values would take orders of magnitude longer, longer than is feasible to make simulation useful. It is important to note that this is not a limitation of the model as such. But rather a limitation of our technology of simulation to approximate the stationary distribution for the regular finite Markov chain in these parameter values. 

Benchmarking the simulator
==========================
We employ Monte Carlo simulations and the relationship between $\widehat{\boldsymbol{D}}$ and $\boldsymbol{RFV}^{n^{\ast}}$ to: (i) cross validate the output from our Python and Matlab codebases; and, (ii) benchmark the accuracy of the simulator against a known transition matrix.\footnote{\ We are, of course, implicitly testing the validity of our convergence criterion.} The computational intensity of producing both $\widehat{\boldsymbol{D}}$ and $\boldsymbol{RFV}^{n^{\ast}}$ increases geometrically with the size of the state space. It is only practical, therefore, to implement both approaches for a relatively small number of agents ($N=50$) and a single look back period ($z=1$). For these parameters, and for each element of $R=\{5,21,45\}$, we generate $1000$ instances of $\boldsymbol{RFV}^{n^{\ast }}$. We then construct the following distance metric: 
\[ATEST=\widehat{\boldsymbol{D}}-\boldsymbol{RFV}^{n^{\ast }} = \sum_{j=0}^{j=N}|\widehat{d}_{j}-rv_{j}^{n^{\ast }}|\]

In Table 4.1 we report the mean, standard deviation, and maximal value of $ATEST$ along with the mean $n^*$. It is apparent that the two platforms deliver results that are essentially the same, and that $\boldsymbol{RFV}^{n^{\ast }}$ very closely approximates $\widehat{\boldsymbol{D}}$.


| Syntax      |
| Syntax      | Description | Test Text     |
| :---        |    :----:   |          ---: |
| Header      | Title       | Here's this   |
| Paragraph   | Text        | And more      |

\begin{table}[h]
\centering
\caption{Benchmarking the simulator}
\begin{tabular}{lccc;{1pt/1pt}ccc} 
\toprule
 & \multicolumn{3}{c} \textbf{Matlab Codebase} & \multicolumn{3}{c} \textbf{Python Codebase} \\ 
\hline
 & \multicolumn{6}{c}{Value of R} \\
 & \textbf{5} & \textbf{21} & \textbf{45} & \textbf{5} & \textbf{21} & \textbf{45} \\
 \hline
Mean of ATEST & 0.0017 & 0.0098 & 0.0078 & 0.0018 & 0.0115 & 0.0087 \\
Std Dev of ATEST & 0.0009 & 0.0017 & 0.0011 & 0.0009 & 0.0019 & 0.0011 \\
Max value ATEST & 0.0061 & 0.0193 & 0.0116 & 0.0057 & 0.0198 & 0.0128 \\
Mean $n^*$ & 6.0010 & 7.8430 & 7.0870 & 6.0010 & 7.8120 & 6.0010 \\
\bottomrule
\end{tabular}
\end{table}

Search algorithm for active policies described
==============================================
We use a two stage procedure to identify the optimal crackdown and refined crackdown policies.  In the first stage, we use a directed search routine with a loose convergence criterion to identify the neighborhood in which the optimal policy lies, and in the second stage we use Monte Carlo methods to generate tight estimates of the cost of policies in the neighborhood. 

The algorithm at the core of the directed search routine is this. For a given crackdown policy at stage $s,$ $CD^{s}=(\mathbb{BE}^{s},\mathbb{R_{GB}}^{s},\mathbb{R_{BB}}^{s})$, a modified policy $CD^{s+1}=(\mathbb{BE}^{s+1},\mathbb{R_{GB}}^{s+1},\mathbb{R_{BB}}^{s+1})$, is generated using a three step procedure: in the first step, $\mathbb{BE}^{s+1}$ is chosen from the set $\{\mathbb{BE}^{s}-5,\mathbb{BE}^{s}-4,\dotsc ,\mathbb{BE}^{s}+5\}$ to minimize the cost of ASB for policy $(\mathbb{BE}^{s+1},\mathbb{R_{GB}}^{s},\mathbb{R_{BB}}^{s})$; in the second step, $\mathbb{R_{GB}}^{s+1}$ is chosen from the set $\{\mathbb{R_{GB}}^{s}-5,\mathbb{R_{GB}}^{s}-4,\dotsc ,\mathbb{R_{GB}}^{s}+5\}$ to minimize the cost of ASB for policy $(\mathbb{BE}^{s+1},\mathbb{R_{GB}}^{s+1},\mathbb{R_{BB}}^{s})$; in the third step, $\mathbb{R_{BB}}^{s+1}$ is chosen from the set $\{\mathbb{R_{BB}}^{s}-5,\mathbb{R_{BB}}^{s}-4,\dotsc ,\mathbb{R_{BB}}^{s}+5\}$ to minimize the cost of ASB for policy $(\mathbb{BE}^{s+1},\mathbb{R_{GB}}^{s+1},\mathbb{R_{BB}}^{s+1})$. For each of the 33 policies considered, we ran a convergence simulation and used it to estimate cost. The search is terminated when the criteria $(E(C|CD^{s})-E(C|CD^{s+1}))/E(C|CD^{s})<.02$ is met. Each of the 1000 simulations was seeded with an initial policy randomly chosen from the set $\{(\mathbb{BE},\mathbb{R_{GB}},\mathbb{R_{BB}})| 0\leq \mathbb{BE}\leq 100, 0\leq \mathbb{R_{GB}}\leq 100, \mathbb{R_{GB}}\leq \mathbb{R_{BB}}\leq 100\}$ and for each we used the directed search algorithm to a generate a terminal policy. From initial policy to terminal policy, the average number of policies evaluated was 165. To identify the neighborhood of the optimal crackdown policy, we ranked the 1000 terminal policies from lowest to highest cost. The first 78 policies in this ranking, and 98 of the first 100, were in the set $\{(\mathbb{BE},\mathbb{R_{GB}},\mathbb{R_{BB}})| 30\leq \mathbb{BE}\leq 39, 29\leq \mathbb{R_{GB}}, 3\leq \mathbb{R_{BB}}\leq 65\}$. This is neighborhood of the optimal policy. 

The search for the optimal refined crackdown policies was analogous to that for the crackdown policy, with the differences being the need for two additional steps in the procedure to search over the two additional parameters, $\mathsf{BE2}$ and $\mathsf{R_{BB1}}$, of the refined crackdown policy. The algorithm for the refined crackdown policy search is, given a refined crackdown policy at stage $s,$ $CD^{s}=(\mathsf{BE}^{s},\mathsf{BE2}^{s},\mathsf{R_{GB}}^{s},\mathsf{R_{BB1}}^{s},\mathsf{R_{BB2}}^{s})$, a modified policy $CD^{s+1}=(\mathsf{BE}^{s+1},\mathsf{BE2}^{s+1},\mathsf{R_{GB}}^{s+1},\mathsf{R_{BB1}}^{s+1},\mathsf{R_{BB1}}^{s+1})$, is generated using a five step procedure analogous to that of the three step procedure of the crackdown policy search. For each of the 55 policies considered, we again ran a convergence simulation and used it to estimate the cost. The termination criteria for the refined crackdown policy search was the same as that used for the crackdown policy search. The 1000 simulations were randomly seeded with initial refined crackdown policies chosen from the set $\{(\mathsf{BE},\mathsf{BE2},\mathsf{R_{GB}},\mathsf{R_{BB1}},\mathsf{R_{BB2}})|0\leq \mathsf{BE}\leq 100, \mathsf{BE}\leq \mathsf{BE2}\leq 100, 0\leq \mathsf{R_{GB}}\leq 100, \mathsf{R_{GB}}\leq \mathsf{R_{BB1}}\leq \mathsf{R_{BB2}}, \mathsf{R_{BB1}}\leq \mathsf{R_{BB2}}\leq 100\}$. From initial policy to terminal policy, the average number of policies evaluated was 440 for each of the 1000 simulations. The neighborhood of the optimal refined crackdown policies was determined in the same way as for the crackdown policy.  

The estimate of cost that comes out of one convergence simulation is, obviously, a random variable. In the neighborhood of the optimal policy, the cost function is so flat that in order to reliably identify the optimum policy, many independent cost estimates are needed. Accordingly, for every policy in the neighborhood of the optimum we ran 150 convergence simulations and calculated the cost of ASB in the steady state for each of them. Our cost estimate is the mean value of these 150 cost estimates. Typically the standard deviation of cost for the 150 simulations is close to 0.20, and hence the standard error of the estimate is close to $0.016=.2/\sqrt{150}$.

Finding optimal refined crackdown
=================================
In the first stage we identify the neighborhood of the optimal refinement by using a straight forward extension of the directed search routine used in Section \ref{section:optimalcrackdown}. For each of the 1000 randomly chosen policies, the directed search routine identifies a candidate policy. In the second stage, we rank the 1000 candidate policies in ascending order of their cost estimates. We then generate tight costs estimates for the first 25 policies in this ranking, running 150 independent convergence simulations for each policy and using the mean cost as our cost estimate for the policy.

You may have noticed that the first two elements in our notation for a refined crackdown policy, $\mathsf{BE}$ and $\mathsf{R_{GB}}$, are the same as those used for the first two elements of a crackdown policy. This reflects a conscious choice to frame a refined crackdown policy as a refinement of a crackdown policy with the same $\mathbb{BE}$ and $\mathbb{R_{GB}}$ in which the bad bin, $BB$, is partitioned into two bins, $BB_1$ and $BB_2$, with different quantities of the enforcement resource. We chose to frame them in this way to highlight the fact the 25 least costly refined crackdown policies we identify in stage 1 of our search procedure are in fact refinements of one of the least costly crackdown policies we identified in the previous section, and presented in Table \ref{table: activecostestimates}. Key results for the 25 refined crackdown policies with the lowest tight cost estimates are reported in Table \ref{table: refinedcostestimates}. Notice that the skeletons of Tables \ref{table: activecostestimates} and \ref{table: refinedcostestimates} are identical. 

\begin{table}[h]
\centering
\caption{The Value of Refinement.}
\label{table: refinedcostestimates}
\begin{threeparttable}
\scriptsize
\begin{tabular}{c c c c c c c c c c c c c}
\toprule
\multicolumn{1}{l}{}              &\multicolumn{1}{l}{}              & \multicolumn{10}{c}{$\mathbf{\mathsf{BE}}$} &\multicolumn{1}{l}{}\\ 
\multicolumn{1}{l}{}              &\multicolumn{1}{l}{}              & \textbf{30}     & \textbf{31}     & \textbf{32}      & \textbf{33}      & \textbf{34}      & \textbf{35}      & \textbf{36}      & \textbf{37}      & \textbf{38}      & \textbf{39} &\multicolumn{1}{l}{} \\ \cline{1-13} 
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{} & & & & & & & & & &\multicolumn{1}{l}{} \\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{29}}  & \begin{tabular}[c]{@{}c@{}}1.07\\ \tiny{9,18}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}0.73\\ \tiny{11}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.62\\ \tiny{6}\end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} &\multicolumn{1}{l}{} \\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{}}    & & & & & & & & & &\multicolumn{1}{l}{} \\
\begin{tabular}[c]{@{}c@{}} \\ $\mathbf{\mathsf{R_{GB}}}$ \end{tabular}  &\multicolumn{1}{c}{\textbf{30}}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}0.68\\ \tiny{2,7,15}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}0.49\\ \tiny{3,4,10}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.42\\ \tiny{1,5,8,13,14,21,22}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.17\\ \tiny{20}\end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} &\multicolumn{1}{l}{} \\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{}}    & & & & & & & & & &\multicolumn{1}{l}{} \\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{31}}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}0.55\\ \tiny{17}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.42\\ \tiny{12,24}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.20\\ \tiny{19,25}\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.24\\ \tiny{16,23}\end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} &\multicolumn{1}{l}{}\\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{}}    & & & & & & & & & &\multicolumn{1}{l}{} \\
\multicolumn{1}{l}{}              &\multicolumn{1}{c}{\textbf{32}}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular}  & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}}XXX\\  \end{tabular} &\multicolumn{1}{l}{} \\ \bottomrule \end{tabular}
\end{threeparttable}
\end{table}

All 25 of the best refined crackdown policies appear in just 11 of the 40 cells in Table \ref{table: refinedcostestimates}, and 13 of them are in cells (32, 30), (33, 30) and (34, 30). In the top line of each cell we have listed the percentage difference in cost between the best refined crackdown policy and crackdown policy for that cell (from Table \ref{table: activecostestimates}). The second line of each cell identifies the refined crackdown policy by its rank---with rank 1 being the lowest cost estimate. Policies 1, 5, 8, 13, 14, 21 and 22 are listed in cell (34, 30). All are refinements of the crackdown policy (34, 30, 56) from Table \ref{table: activecostestimates}, and all produce a lower cost. This is not surprising -- with more instruments to control ASB we expect to see a lower cost of ASB. What may be surprising is that the reduction in cost is relatively small --- that is what the entry in the first line of each cell addresses. The cost estimate for the refined crackdown policy with rank 1 is only 0.41\% lower than the costs estimate for optimal crackdown policy (34, 30, 56). 

In Table \ref{table: stderrorscostestimates} we report the policies that are ranked 1, 2 and 3, our costs estimates for them, and the standard errors of the cost estimates. When we look at the policies themselves we see that the refinements make intuitive sense. The lowest cost refined crackdown policy is $(\mathsf{BE},\mathsf{R_{GB}},\mathsf{SBE},\mathsf{R_{BB1}},\mathsf{R_{BB2}})=(34,30,44,47,64)$ and it is a refinement of crackdown policy $(\mathbb{BE},\mathbb{R_{GB}},\mathbb{R_{BB}})=(34,30,56)$. When crackdown policy $(34,30,56)$ is used, 56 units of enforcement resource are deployed in the bad bin, $\{v^{t-1}|34 < v^{t-1}\leq 100\}$. When refined crackdown policy $(34,30:44,47,64)$ is used the bad bin is partitioned into a two two bins, the first is $\{v^{t-1}|34 < v^{t-1}\leq 44\}$ and the second is $\{v^{t-1}|44 < v^{t-1}\leq 100\}$, and quantity of enforcement resources deployed in the first is 47, somewhat less 56, and quantity deployed in the second is 64, somewhat greater than 56. In the bad bin, the proximate objective of policy is to drive violations down and back into $GB$, and it makes intuitive sense that the quantity of resources needed to do that efficiently is smaller when violations are close to the lower bound of $BB$ than when violations are close to the upper bound.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.80\textwidth]{Penultimate Draft/Images/Fig 17.png}
    \caption{Cross sections of costs in the neighborhood of $RCD\#1$}
    \label{fig:RCD}
\end{figure}

In Section \ref{section:optimalcrackdown}, to identify the optimal crackdown policy, we generated a
tight cost estimate for every policy in the neighborhood that we identified using the stage one directed search routine. That approach is not feasible for refined crackdown policies. We can use the directed search routine to identify the neighborhood of the optimum, but there are so many policies in that neighborhood that it is not feasible to generate a tight cost estimate for each of them. In addition, in three of the five relevant dimensions, $\mathsf{SBE}$, $\mathsf{R_{BB_{1}}}$ and $\mathsf{R_{BB_2}}$, the cost function is so flat that it would take thousands of convergence simulations to get cost estimates that would allow us to identify the optimal policy. 

Nevertheless, the results reported in Figure \ref{fig:RCD} suggest that the cost of ASB for the optimal policy is very close to the cost estimates in Table \ref{table: stderrorscostestimates}. The cost estimates reported in the figure are based on 50 independent convergence simulations for each policy examined (the standard errors of these estimates are roughly 0.03). In each panel, cost estimates are reported for 19 policies centered on policy \#1: four of the five parameters of the policy are fixed at their values in policy \#1, and the fifth varies up and down from its value in policy \#1 by 9 units. The asterisk indicates policy \#1, and the dot the policy with the lowest cost estimate. There is considerable curvature in the cost function when $\mathsf{R_{GB}}$ and $\mathsf{BE}$ vary, which means that the optimal values of these parameters are much easier to pin down, and that is exactly what we saw in Table \ref{table: stderrorscostestimates}. But over the range of values examined for parameters $\mathsf{SBE}$, $\mathsf{R_{BB_{1}}}$ and $\mathsf{R_{BB_{2}}}$, the cost function is very flat and it is much more difficult to pin down the optimal values of theses parameters. This flatness reflects the fact that there are many ways to extinguish $BB$, and, it does matter much in terms of costs precisely how that is achieved because so little time is spent there. The inserts in each panel present the same information but the scale on the vertical axis is different. The more fine grained scale allows us to see how the cost estimates for the refined crackdown policies compare to our best estimate of the minimum cost achievable with the simpler crackdown policy---the dotted reference line in each insert represents that cost.
