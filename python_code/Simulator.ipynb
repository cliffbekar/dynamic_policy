{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebase for \"A Dynamic Theory of Deterrence and Compliance\"\n",
    "\n",
    "Format: JupyterLab Notebook\n",
    "\n",
    "Kernel: Python3\n",
    "\n",
    "Code for all results and figures. \n",
    "\n",
    "*Note, to use this file user must first set the local path to store results of computation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    import libraries and set display options \n",
    "\"\"\"\n",
    "import sys\n",
    "import matplotlib.collections\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import math as math\n",
    "import quantecon as qe\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numba import jit\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.stats import entropy\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "path_data = \n",
    "path_code = \n",
    "path_figs = \n",
    "\n",
    "# aesthetic settings\n",
    "import warnings\n",
    "sys.setswitchinterval(1500)\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    baseline model parameters for kernel\n",
    "    \n",
    "\"\"\"\n",
    "# main parameters\n",
    "agents = 100\n",
    "Z = 1, 2\n",
    "\n",
    "# distribution parameters\n",
    "ḡ , σ = 0.6 , 0.2\n",
    "\n",
    "# Bayesian priors\n",
    "α, β = 1, 0.25\n",
    "\n",
    "# costs\n",
    "ρ, λ = 2 , 5\n",
    "\n",
    "# domain of analysis for policy\n",
    "F = 1\n",
    "R_low, R_high = 0, agents+1\n",
    "\n",
    "# benchmarking parameters\n",
    "block = 50000\n",
    "checks, C = 5, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    functions used by simulations \n",
    "\"\"\"\n",
    "\n",
    "# apprehension function, min catch\n",
    "@jit(nopython=True)\n",
    "def Apprehend(v, R):\n",
    "    γ = 0.80 \n",
    "    if v == 0:\n",
    "        prob_a = γ\n",
    "    else:\n",
    "        prob_a = γ*min(1, R/v)\n",
    "    return prob_a\n",
    "\n",
    "# apprehension function, exponential\n",
    "@jit(nopython=True)\n",
    "def Apprehend_exp(v, R):\n",
    "    γ = 0.80  \n",
    "    ϵ = 8\n",
    "    if v == 0:\n",
    "        prob_a = γ\n",
    "    else:\n",
    "        prob_a = γ*(1-(1/pow(ϵ,(R/v))))\n",
    "    return prob_a\n",
    "\n",
    "# create a 'criminal opportunity'\n",
    "def Opportunity():\n",
    "    lower, upper = 0, 1\n",
    "    ḡ , σ = 0.6, 0.2\n",
    "    return max(0, np.random.normal(ḡ, σ)) \n",
    "\n",
    "# create a distribution of 'criminal opportunities'\n",
    "@jit(nopython=True)\n",
    "def Criminal_opps(μ, σ, agents, T):\n",
    "    g = np.zeros((T,agents))\n",
    "    for i in range(T):\n",
    "        g[i] = [max(0, np.random.normal(ḡ, σ)) for agent in range(agents)]\n",
    "    return g\n",
    "\n",
    "# create a distribution of 'criminal opportunities'\n",
    "@jit(nopython=True)\n",
    "def Criminal_opps_uniform(low, high, agents, T):\n",
    "    g = np.zeros((T,agents))\n",
    "    for i in range(T):\n",
    "        g[i] = [np.random.uniform(low, high) for agent in range(agents)]\n",
    "    return g\n",
    "\n",
    "def Simulation(criminal_opps, R, F, Z):\n",
    "# agent priors\n",
    "    α,β = 1, 0.25\n",
    "    init_period = 100\n",
    "# tracking arrays for endogenous variables of interest\n",
    "    a_t,v_t, g_t, q_t = [], [], [], []\n",
    "# seed for priors\n",
    "    append_v = v_t.append\n",
    "    append_a = a_t.append\n",
    "    append_g = g_t.append\n",
    "    append_q = q_t.append\n",
    "    for t in range(Z):\n",
    "        v = np.random.randint(1,agents)\n",
    "        a = np.random.randint(0,v)\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "# main sim loop, starts at T = 500   \n",
    "    for t in range(len(criminal_opps)):\n",
    "        a, v, g, q = 0, 0, 0, 0\n",
    "        q = (α + np.sum(a_t[-Z:]))/(α + β + np.sum(v_t[-Z:]))\n",
    "        for g_i in criminal_opps[t]:\n",
    "            if  q <= g_i/F:\n",
    "                v = v + 1\n",
    "                g = g + g_i\n",
    "        a = stats.binom.rvs(v, Apprehend(v, R))\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "        append_g(g)\n",
    "        append_q(q)\n",
    "    \n",
    "    return np.array(v_t[init_period+Z:]), np.array(a_t[init_period+Z:]), np.array(g_t[init_period+Z:]), np.array(q_t[init_period+Z:])\n",
    "\n",
    "def Simulation_exp(criminal_opps, R, F, Z):\n",
    "# agent priors\n",
    "    α,β = 1, 0.25\n",
    "    init_period = 100\n",
    "# tracking arrays for endogenous variables of interest\n",
    "    a_t,v_t, g_t, q_t = [], [], [], []\n",
    "# seed for priors\n",
    "    append_v = v_t.append\n",
    "    append_a = a_t.append\n",
    "    append_g = g_t.append\n",
    "    append_q = q_t.append\n",
    "    for t in range(Z):\n",
    "        v = np.random.randint(1,agents)\n",
    "        a = np.random.randint(0,v)\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "# main sim loop, starts at T = 500   \n",
    "    for t in range(len(criminal_opps)):\n",
    "        a,v,g = 0,0,0\n",
    "        q = (α + np.sum(a_t[-Z:]))/(α + β + np.sum(v_t[-Z:]))\n",
    "        for g_i in criminal_opps[t]:\n",
    "            if  q <= g_i/F:\n",
    "                v = v + 1\n",
    "                g = g + g_i\n",
    "        a = stats.binom.rvs(v, Apprehend_exp(v, R))\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "        append_g(g)\n",
    "        append_q(q)\n",
    "    \n",
    "    return np.array(v_t[init_period+Z:]), np.array(a_t[init_period+Z:]), np.array(g_t[init_period+Z:]), np.array(q_t[init_period+Z:])\n",
    "\n",
    "def Simulation_het_q(criminal_opps, R, F, Z):\n",
    "# agent priors\n",
    "    α,β = 1, 0.25\n",
    "    init_period = 100\n",
    "# tracking arrays for endogenous variables of interest\n",
    "    a_t,v_t, g_t, q_t = [], [], [], []\n",
    "# seed for priors\n",
    "    append_v = v_t.append\n",
    "    append_a = a_t.append\n",
    "    append_g = g_t.append\n",
    "    append_q = q_t.append\n",
    "    for t in range(Z):\n",
    "        v = np.random.randint(1,agents)\n",
    "        a = np.random.randint(0,v)\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "# main sim loop, starts at T = 500   \n",
    "    for t in range(len(criminal_opps)):\n",
    "        a,v,g = 0,0,0\n",
    "        q = (α + np.sum(a_t[-Z:]))/(α + β + np.sum(v_t[-Z:]))\n",
    "        for g_i in criminal_opps[t]:\n",
    "            q_i = q + np.random.uniform(-0.2,0.2)\n",
    "            if q_i > 1:\n",
    "                q_i = 1\n",
    "            if q_i < 0:\n",
    "                q_i = 0\n",
    "            if  q_i <= g_i/F:\n",
    "                v = v + 1\n",
    "                g = g + g_i\n",
    "        a = stats.binom.rvs(v, Apprehend(v, R))\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "        append_g(g)\n",
    "        append_q(q)\n",
    "    \n",
    "    return np.array(v_t[init_period+Z:]), np.array(a_t[init_period+Z:]), np.array(g_t[init_period+Z:]), np.array(q_t[init_period+Z:])\n",
    "\n",
    "def Crackdown(criminal_opps, BE, RGB, RBB, F, Z):\n",
    "# agent priors\n",
    "    T = len(criminal_opps)\n",
    "    α,β = 1, 0.25\n",
    "    r = RGB\n",
    "# tracking arrays for endogenous variables of interest\n",
    "    a_t,v_t, g_t, r_t = [], [], [],[]\n",
    "# seed for priors\n",
    "    append_v = v_t.append\n",
    "    append_a = a_t.append\n",
    "    append_g = g_t.append\n",
    "    append_r = r_t.append\n",
    "    for t in range(Z):\n",
    "        v = np.random.randint(1,agents)\n",
    "        a = stats.binom.rvs(v, Apprehend(v, r))\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "        append_r(r)\n",
    "# main sim loop, starts at T = 500   \n",
    "    for t in range(len(criminal_opps)):\n",
    "        if t > 0:\n",
    "            if v >= BE:\n",
    "                r = RBB\n",
    "            else:\n",
    "                r = RGB \n",
    "        else:\n",
    "            r = RGB\n",
    "        a,v,g = 0,0,0\n",
    "        q = (α + np.sum(a_t[-Z:]))/(α + β + np.sum(v_t[-Z:]))\n",
    "        for g_i in criminal_opps[t]:\n",
    "            if  q <= g_i/F:\n",
    "                v = v + 1\n",
    "                g = g + g_i\n",
    "        a = stats.binom.rvs(v, Apprehend(v, r))\n",
    "        append_v(v)\n",
    "        append_a(a)\n",
    "        append_g(g)\n",
    "        append_r(r)\n",
    "    \n",
    "    return np.array(v_t[-T:]), np.array(a_t[-T:]), np.array(g_t[-T:]), np.array(r_t[-T:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space Production and Transition Matrices\n",
    "\n",
    "The first function produces, for a given number of agents, the State Space of the model ($\\boldsymbol{Z}$) from Section 2.3.\n",
    "\n",
    "The second function produces the conditional probabilities establishing Proposition 3 on page 11.\n",
    "\n",
    "The third function takes the probabilities from above as arguments and produces the complete transition matrix for the model ($\\boldsymbol{T}$ from page 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Produce State space\n",
    "        parameters: agents\n",
    "        returns: S, Shat, s\n",
    "\"\"\"\n",
    "def State_space(agents: int):\n",
    "    w = np.sum(np.arange(1,agents+2))\n",
    "    s = np.zeros(w, dtype=int)\n",
    "    R̂ = np.empty(w, dtype=object)\n",
    "    Ŝ = np.empty(w, dtype=object)\n",
    "    S = [[(i,j) for j in range(agents+1)] for i in range(agents+1)]\n",
    "    l=0\n",
    "\n",
    "    for i in range(agents+1):\n",
    "        for j in range(agents+1):\n",
    "            if i >= j:\n",
    "                k = np.arange(i+1)\n",
    "                S[i][j] = (i,j)\n",
    "                Ŝ[l] = (1+i-j+np.sum(k),i,j)\n",
    "                s[l] = 1+i-j+np.sum(k)\n",
    "                l=l+1\n",
    "            if i < j:\n",
    "                S[i][j] = (i,0)\n",
    "    Ŝ = sorted(Ŝ, key=lambda tup: tup[0])\n",
    "    s = sorted(s)\n",
    "    return S, Ŝ, s\n",
    "\n",
    "\"\"\"\n",
    "    Produce M1 and M2 matrices to speed analysis\n",
    "        parameters: agents, F\n",
    "        returns: M1, M2\n",
    "\"\"\"\n",
    "\n",
    "def TM_matrices(agents: int, F:float):\n",
    "# parameters\n",
    "    ḡ = 0.60\n",
    "    σ = 0.20\n",
    "    α,β = 1,0.25\n",
    "# tracking arrays/vectors\n",
    "    S, Ŝ, s = State_space(agents)\n",
    "    w = len(s)\n",
    "    A = np.zeros(w)\n",
    "    π = np.zeros(w)\n",
    "    M1 = np.zeros((w,w))\n",
    "    M2 = np.zeros((w,w))\n",
    "    \n",
    "# main loop\n",
    "    for state_1 in range(w):\n",
    "        v = Ŝ[state_1][1]\n",
    "        a = Ŝ[state_1][2]\n",
    "        q = (α + a)/(α + β + v)\n",
    "        π[state_1] = 1 - norm.cdf(F*q-ḡ,0,σ)\n",
    "        for state_2 in range(w):\n",
    "            v = Ŝ[state_2][1]\n",
    "            a = Ŝ[state_2][2]\n",
    "            v̂ = math.factorial(v)\n",
    "            â = math.factorial(a)\n",
    "            d̂ = math.factorial(v-a)\n",
    "            M1[state_1,state_2] = binom.pmf(v,agents,π[state_1])\n",
    "            M2[state_1,state_2] = v̂/(â*d̂)\n",
    "    return M1, M2\n",
    "\n",
    "\"\"\" \n",
    "    Produce transition matrix for all s to s'\n",
    "        s = numbered states\n",
    "\"\"\"\n",
    "def Transition_matrix(agents: int, M1, M2, R: int, F:float, switch:bool):\n",
    "# tracking arrays/vectors\n",
    "    S, Ŝ, s = State_space(agents)\n",
    "    w = len(s)\n",
    "    T̂ = np.zeros((w,w))\n",
    "    \n",
    "# main loop\n",
    "    for state_1 in range(w):\n",
    "        for state_2 in range(w):\n",
    "            v = Ŝ[state_2][1]  \n",
    "            a = Ŝ[state_2][2]\n",
    "            d = v - a\n",
    "            if switch:\n",
    "                Â = Apprehend(v, R[state_1])\n",
    "            else:\n",
    "                Â = Apprehend(v, R[state_1])\n",
    "            M3 = math.pow(Â, a)*math.pow(1-Â, d)\n",
    "            T̂[state_1,state_2] = M1[state_1,state_2]*M2[state_1,state_2]*M3\n",
    "    return T̂\n",
    "\n",
    "def Compute_cost(agents: int, R:int , F:float):\n",
    "# parameters\n",
    "    δ, λ = 2, 5\n",
    "    ḡ = 0.60\n",
    "    σ = 0.20\n",
    "    α,β = 1,0.25\n",
    "# tracking arrays/vectors\n",
    "    S, Ŝ, s = State_space(agents)\n",
    "    w = len(s)\n",
    "    A = np.zeros(w)\n",
    "    π = np.zeros(w)\n",
    "    Ĉ = np.zeros(w)\n",
    "    \n",
    "# main loop\n",
    "    for state in range(w):\n",
    "        v = Ŝ[state][1]\n",
    "        a = Ŝ[state][2]\n",
    "        q = (α + a)/(α + β + v)\n",
    "        π[state] = 1 - norm.cdf(F*q-ḡ, 0, σ)\n",
    "        A[state] = Apprehend(π[state]*agents, R[state])\n",
    "        Ĉ[state] = δ*R[state] + (λ-1)*agents*π[state] + agents*F*π[state]*A[state]\n",
    "    return Ĉ  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table A.1\n",
    "\n",
    "First cell produces the transition matrix of the finite regular Markov chain. Using the TM we produce the associated Markov chain (MC) and its associated stationary distribution ($\\bar{S}$).\n",
    "\n",
    "The following cell produces the results from Table A.1 by comparing the estimated distribution of violations ($\\boldsymbol{\\hat{D}}$) against $\\bar{S}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    produce TM and SD in order to benchmark simulation\n",
    "            state space for:\n",
    "                    N = 50, Z=1\n",
    "\"\"\"\n",
    "# paramaters for benchmarking exercise\n",
    "switch = True\n",
    "agents, Z = 50, 1\n",
    "R_low, R_high = 0, agents+1\n",
    "# variables used for benchmark exercise\n",
    "R = np.ones(np.sum(np.arange(1,agents+2)), dtype=int)\n",
    "\n",
    "# \"control\" variables\n",
    "ex_v = np.zeros(R_high-R_low)\n",
    "ex_a = np.zeros(R_high-R_low)\n",
    "std_v = np.zeros(R_high-R_low)\n",
    "# vectors, arrays, storage objects\n",
    "S̄ = np.empty(R_high-R_low, dtype=object)\n",
    "V̂ = np.empty(R_high-R_low, dtype=object)\n",
    "Â = np.empty(R_high-R_low, dtype=object)\n",
    "Ĉ = np.empty(R_high-R_low, dtype=object)\n",
    "MC = np.empty(R_high-R_low, dtype=object)\n",
    "\n",
    "# create the ordered state space\n",
    "S, Ŝ, s = State_space(agents)\n",
    "max_v = np.zeros(len(s))\n",
    "max_a = np.zeros(len(s))\n",
    "for state in range(len(s)):\n",
    "    max_v[state] = Ŝ[state][1]\n",
    "    max_a[state] = Ŝ[state][2]\n",
    "\n",
    "# create transition matrices\n",
    "M1 , M2 = TM_matrices(agents, F)\n",
    "\n",
    "# main loop\n",
    "for r in range(R_low, R_high):\n",
    "    index = 0\n",
    "    a, v = 0, 0\n",
    "    R.fill(r)\n",
    "    TM = Transition_matrix(agents, M1, M2, R, F, switch)\n",
    "    MC[r] = qe.MarkovChain(TM)\n",
    "    S̄[r] = MC[r].stationary_distributions\n",
    "    V̂[r] = S̄[r]*max_v\n",
    "    Â[r] = S̄[r]*max_a\n",
    "    ex_v[r] = np.sum(V̂[r])\n",
    "    ex_a[r] = np.sum(Â[r])\n",
    "    std_v[r] = np.std(V̂[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0087\n",
      "0.0011\n",
      "0.0134\n",
      "6.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        Benchmarking: Run simulation to converge for comparison with results from cell above\n",
    "                        \n",
    "                        Note: need to set r = 5, 21, 45 to reproduce Table A.1\n",
    "\"\"\"\n",
    "runs, block = 1000, 50000\n",
    "agents = 50\n",
    "Z = 1\n",
    "r = 45\n",
    "C, checks = 0.01, 5\n",
    "TEST = np.zeros(runs)\n",
    "\n",
    "# translate SD from TM into same format as DD1\n",
    "#      DD1 runs from 0 to agents+1\n",
    "#      state space of SD is much larger\n",
    "DD3 = np.zeros(agents+1)\n",
    "for i in Ŝ:\n",
    "    DD3[i[1]] = DD3[i[1]] + S̄[r][0][i[0]-1]\n",
    "\n",
    "time_to_converge = 0\n",
    "for run in range(runs):\n",
    "    v_μ = 0\n",
    "    converged, passed = False, 0\n",
    "    DD1 = np.zeros(agents+1)\n",
    "    DD2 = np.zeros(agents+1)\n",
    "    criminal_opps = Criminal_opps(ḡ,σ,agents,block)\n",
    "    v_t, a_t, g_t = Simulation(criminal_opps,r,F,Z)\n",
    "    v, a, g = 0, 0, 0\n",
    "    v = np.append(v, v_t)\n",
    "    a = np.append(a, a_t)\n",
    "    g = np.append(g, g_t)\n",
    "    for i in v:\n",
    "        DD1[i] = DD1[i] + 1\n",
    "    while not converged:\n",
    "        time_to_converge = time_to_converge + 1\n",
    "        criminal_opps = Criminal_opps(ḡ,σ,agents,block)\n",
    "        v_t, a_t, g_t = Simulation(criminal_opps,r,F,Z)\n",
    "        v = np.append(v, v_t)\n",
    "        a = np.append(a, a_t)\n",
    "        g = np.append(g, g_t)\n",
    "        for i in v:\n",
    "            DD2[i] = DD2[i] + 1    \n",
    "        if np.abs(np.sum((DD2/np.sum(DD2)) - (DD1/np.sum(DD1)))) < C:\n",
    "            passed = passed + 1\n",
    "            if passed > checks:\n",
    "                converged = True\n",
    "        DD1 = DD2[:]\n",
    "    DD1 = DD1/np.sum(DD1)\n",
    "    TEST[run] = np.sum(np.abs(DD1-DD3))\n",
    "    \n",
    "x = np.mean(TEST)\n",
    "y = np.std(TEST)\n",
    "z = np.max(TEST)\n",
    "\n",
    "np.save(path+'TEST.npy', TEST)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "print(\"%0.4f\" % x),print(\"%0.4f\" % y),print(\"%0.4f\" % z),print(\"%0.4f\" % ((time_to_converge+1)/1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
