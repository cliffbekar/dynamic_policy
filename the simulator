The simulator
=============
The number of states in $\boldsymbol{Z}$ for $z=1$ and $N\leq 100$ is manageable, meaning it is feasible to produce the associated transition matrix $\boldsymbol{T}$ and use it to directly compute $\boldsymbol{D}$. But for $z>1$ and larger $N$ we must use numerical simulation methods. We now describe the simulation and our benchmarking efforts.

In each period of the simulation: Initiating a simulation requires values for: the quantity of enforcement resources ($R$); people's look back horizon ($z$); the number of potential violators ($N$); the mean ($\mu$) and standard deviation ($\sigma$) of the private gains from committing a violation; the probability a violator is apprehended if investigated ($\gamma$); the utility penalty paid by apprehended violators ($F$); and, the population's Bayesian priors regarding apprehension ($\alpha $, $\beta $). Unless noted, we set these parameters according to our baseline model.

* Every potential violator is assigned a $g$ drawn randomly from $\phi(g)$.
*Using the current z-history, a common subjective probability of apprehension $q$ is calculated.
* People choose to violate if and only if their $g^{t}_{i}\geq q^{t} \cdot F$.
* Violators are apprehended with probability $P(R^{t}, v^{t})$, where $v^{t}$ is the number of violators in the current period.
* The number of violations, apprehensions, and the sum of the gains from violating, the sum of the realized draws on $\phi(g)$ for those who violated, are recorded.
* The frequency distribution of violations and apprehensions is calculated for the current period and recorded.
* The z-history is updated and the simulation moves to the next period.

We calculate the frequency distribution of violations and apprehensions. The frequency distribution of violations is $\boldsymbol{FV}(nv_{0}^{t},nv_{1}^{t},\dotsc ,nv_{N}^{t})$, where $nv_{j}^{t}$ is the number of instances in the first $t$ periods of the simulation in which there were exactly $j$ violations. The frequency distribution of apprehensions is $\boldsymbol{FA}(na_{0}^{t},na_{1}^{t},\dotsc ,na_{N}^{t})$, where $na_{j}^{t}$ is the number of instances in the first $t$ periods of the simulation in which there were exactly $j$ apprehensions. 

The simulation proceeds in blocks of $50,000$ ticks. At the end of each block $n$ we use $\boldsymbol{FV}$ to calculate the relative frequency distribution of violations $\boldsymbol{RFV}^{n}=(rv_{0}^{n},\dotsc ,rv_{N}^{n})$, where $rv_{j}^{n}$ is the proportion of periods in which there were exactly $j$ violations in the first $50000\cdot n$ periods of the simulation. As $n$ approaches infinity  $\boldsymbol{RFV}^{n}$ approaches $\widehat{\boldsymbol{D}}$. The question is: How large must $n$ be to ensure that $\boldsymbol{RFV}^{n}$ is a \textit{good} approximation of $\widehat{\boldsymbol{D}}$? To answer this we need a convergence criterion. 

At the end of each simulation block, for all $n\geq 2$, we calculate the following test statistic:
\[TEST^{n}=\sum_{j=0}^{j=N}|rv_{j}^{n}-rv_{j}^{n-1}|\]
$TEST^{n}$ is a measure, decreasing in $n$, of the \textit{distance} between $\boldsymbol{RFV}^{n}$ and $\boldsymbol{RFV}^{n-1}$. $TEST^{n}$ does not decline monotonically, which complicates any test of convergence. After extensive experimentation we settled on the following criterion: we require that $TEST^{n} \leq 0.01$ for $5$ consecutive blocks of $n$. Denoting the minimum $n$ for which the convergence criterion is satisfied by $n^{\ast }$ yields our approximation of the stationary distribution, $\boldsymbol{RFV}^{n^{\ast }} \sim \widehat{\boldsymbol{D}}$.\footnote{\ An important confounding issue for finding $n^*$ is that the \textit{shape} of $\widehat{\boldsymbol{D}}$ changes dramatically as $R$ changes. For low values of $R$, $\widehat{\boldsymbol{D}}$ is unimodal and right skewed, for high values unimodal and left skewed, for intermediate values it becomes bimodal. This renders $n^*$ non-monotonic in R (see the last row of Table A.1).}

Our convergence simulator has some limitations over certain ranges of some key parameter values, in particular those that comprise the composite parameter $\frac{\sigma}{(\gamma \cdot F)}$ defined in Section \ref{section:CLIFFSET}. If $\frac{\sigma}{(\gamma \cdot F)}$ is small the $n$ required to achieve a good approximation of the stationary distribution is actually much larger than the $n^{\ast}$ generated from the convergence criteria. This is because of the persistence of the q-attractors. For these parameterizations the simulator is highly sensitive to initial conditions, staying for a very long time (sometimes the entire simulation) in only one of the q-attractors and rarely (if ever for some convergence simulations around the cliff) transitions to the other q-attractor, even though the stationary distribution is bi-model in these parameter values. This yields a poor estimate of the stationary distribution even though the convergence simulator may have taken a very long time to converge. More accurate estimations of the stationary distribution for such parameter values would take orders of magnitude longer, longer than is feasible to make simulation useful. It is important to note that this is not a limitation of the model as such. But rather a limitation of our technology of simulation to approximate the stationary distribution for the regular finite Markov chain in these parameter values. 
